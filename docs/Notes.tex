\documentclass[a4,11pt,twoside,leqno]{report}
\setlength{\topmargin}{0cm} \setlength{\textheight}{215mm}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}\setlength{\textwidth}{160mm}

\usepackage{amsfonts}
\usepackage{amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage[latin1]{inputenc}
\usepackage[mathscr]{euscript}
\usepackage{tabularx}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem*{ex}{Example}
\numberwithin{equation}{section}
\newtheorem*{Claim}{Claim}

\newcommand{\R}{\mathbb{R}}

\begin{document}

\title{Notes for Research Project on Ranking}
\author{Sinan Aksoy, Fan Chung, Olivia Simpson}


\maketitle

\tableofcontents


\chapter{Background}

Here, we give some background on our problem. We start by listing some of the basic properties of the matrices we will be considering. Then, we briefly consider two popular ranking methods: the Perron eigenvalue method and the logarithmic least squares method. 

\section{Relevant Matrices}
\subsection{Positive matrices}


\begin{thm}[Perron-Frobenius \cite{analytic} \cite{hornandjohnson}] Let $A>0$. Then:
\begin{enumerate}
\item There is a positive (column) vector $w$ such that $Aw=\rho(A)w$. We call $w$ the {\em right Perron vector} of $A$. Similarly, there is a positive (row) vector $v$ such that $vA=\rho(A)v$. We call $v$ the {\em left Perron vector} of $A$. 
\item The right Perron vector $w$ is orthogonal to all other eigenvectors in right eigenspace; similarly, $v$ is orthogonal to all other eigenvectors in left eigenspace. 
\item $\rho(A)$ is an algebraically (and hence geometrically) simple eigenvalue of $A$. 
\item $\rho(A)$ is the unique eigenvalue of maximum modulus. 
\item $\rho(A)$ is bounded above by the maximum row sum of $A$ and from below by the minimum row sum of $A$. 
\end{enumerate}

\end {thm}

As an immediate corollary, we can state limiting behavior of positive matrices A in terms of the right and left Perron vectors:

\begin{cor}[\cite{hornandjohnson}] Let $w$ and $v$ be the right and left Perron vectors of $A>0$. Assume $w$ and $v$ are scaled so that $wv=1$ Then:

$$\lim_{m \rightarrow \infty} \Big(\frac{A}{\rho(A)}\Big)^m=vw$$

\end{cor}

Lastly, we note:

\begin{thm}[\cite{analytic}] Let $A>0$. The largest eigenvalue of $A$, $\rho(A)$, increases as any element $a_{ij}$ increases. 
\end{thm}

\subsection{Positive reciprocal matrices}

\begin{defn} $A$ is a positive reciprocal matrix if $a_{ij}>0$ and $a_{ji}=\frac{1}{a_{ij}}$. 

\end{defn}

\begin{thm}[\cite{analytic}] The eigenvalues of a positive reciprocal matrix satisfy:

$$\displaystyle \sum_{\substack{{j,k}\\j\not=k}} \lambda_j\lambda_k=0$$

\end{thm}


\subsection{Positive consistent (reciprocal) matrices}

\begin{defn} A matrix $A$ is said to be consistent if $a_{ij}a_{jk}=a_{ik}$ for all $i,j,k$. In other words, a positive reciprocal matrix is consistent when pairwise dominance relations are transitive. \\

\noindent {\bf Comment}: Using the above definition, along with the fact that $a_{ji}=1/{a_{ij}}$, the consistency condition becomes $a_{ij}a_{jk}a_{ki}=1$. So, one can also define consistency in a graph theoretic sense: if we think of our matrix $A$ as describing a weighted adjacency matrix, then consistency means that the product of the edge weights in any 3-cycle is 1. \\

\noindent {\bf Comment}: Note that all positive consistent matrices are necessarily reciprocal matrices. Assuming $A$ is positive, the definition of consistency implies $a_{ii}=1$, which, in turn, implies $a_{ij}=a_{ii}/a_{ji}=1/a_{ji}$. Thus, a positive, consistent matrix is necessarily a consistent, positive reciprocal matrix (but the converse is not necessarily true). \\

\noindent {\bf Fact}: If $A$ is a consistent positive reciprocal matrix, then the right and left Perron vectors of $A$, $w$ and $v$, are reciprocals of each other up to a multiplicative factor. That is, there exists $c\in \mathbb{R}$ such that:

$$w_i=c\Big(\frac{1}{v_i}\Big)$$

 Next, we note that consistency can be characterized in terms of the largest eigenvalue of a reciprocal matrix:

\begin{thm}[\cite{analytic}] Let $A$ be a positive reciprocal matrix. Then $A$ is consistent if and only if $\rho(A)=n$.

\end{thm}

Consistent, positive reciprocal matrices also satisfy:

\begin{thm}[\cite{analytic}] Let $A$ be a consistent, positive reciprocal matrix. Then:

$$A^k=n^{k-1}A$$

\end{thm}

A natural question that arises is how to measure consistency in matrices that aren't consistent. Several metrics have been proposed - we mention a popular ''consistency index'' below defined by Saaty \cite{analytic}:

\begin{defn} The Consistency Index (C.I.) of a positive reciprocal matrix $A$ is defined as:

$$\mbox{C.I.}=\frac{\rho(A)-n}{n-1}$$

\end{defn}

\noindent {\bf Open Conjecture}: Let $A\in M_n$ be a positive, reciprocal matrix with $n\geq 4$. The right and left Perron vectors of $A$ are reciprocals of each other (up to a multiplicative factor) if and only if $A$ is consistent.  (Note: for $n=2$, any positive reciprocal matrix is consistent. For $n=3$, it has been proven that regardless of consistency, the right and left eigenvectors are reciprocals of each other. As far as I can tell, the conjecture is still unproven for $n\geq4$.) \\

\noindent {\bf Comment}: While it may not be helpful, we note the above conjecture can be alternatively stated in the language of graph theory: Let $A$ be the weighted adjacency matrix of the complete weighted digraph on $n$ vertices for $n\geq 4$. The products of the edge weights of every 3-cycle is 1 if and only if the right and left Perron vectors of $A$ are reciprocals (up to a multiplicative factor) of each other.

\end{defn}

\section{Two ranking methods}

\subsection{Perron eigenvalue method}

Saaty advocates use of the Perron eigenvalue method in which the ranking is given by the Perron vector of $A$. Saaty makes two key points to justify this method. The first is that, when $A$ is consistent, the Perron vector $w$ satisfies:

$$a_{ij}=\frac{w_i}{w_j}$$

This can be easily proved. Since every elements of $A$ can be determined from the first row of $A$, $A$ has rank one and exactly one nonzero eigenvalue. Using the aforementioned fact that $A^2=nA$, and denoting the columns of $A$ as $[a_1,a_2,\dots,a_n]$ we can see:

$$\begin{array}{lcl} A^2&=&A[a_1,a_2,\dots,a_n] \\ &=&[Aa_1,Aa_2,\dots,Aa_n] \\ &=& [na_1,na_2,\dots,na_n] \\ &=& nA\end{array}$$

which tells us the columns of $A$ are all scalar multiples of the (unique) dominant eigenvector. So, for column $a_k$, the Perron vector $w$ satisfies, for some scalar $c$:

$$\begin{array}{lcl}w_i&=&ca_{ik}\\ w_j&=&ca_{jk} \end{array}$$

which, if we take the ratio and use the consistency definition, yields the desired result: 

$$a_{ij}=\frac{w_i}{w_j}$$

The second key justification Saaty makes in favor of using the Perron eigenvalue method concerns perturbation. Namely, he shows that small changes in the entries of the matrix result in small changes in the Perron vector. While these two arguments seem to be the most compelling, Saaty and others also have other arguments, including empirical arguments, on why the Perron vector should be used in ranking. 

\subsubsection{Asymmetry in rankings derived from right and left Perron vectors}

While Saaty and others have traditionally used the right Perron vector for ranking, there seems to be no justification for why the left eigenvector should not be used in place of the right eigenvector. Larger values in the right Perron vector signify a higher ranking for that item whereas smaller values in the left Perron vector signify a higher ranking for that item. In the case where $A$ is consistent, given the above mentioned reciprocal relationship between the right and left Perron vectors, both vectors will lead to the same ranking. However, if $A$ is inconsistent, then the right and left Perron vectors can lead to different rankings. In a recent survey of the Analytic Hierarchy Process, the author listed this issue as the largest theoretical dispute in the field. We give an example below from \cite{johnson} to that illustrates the rank reversal issue:

$$A=\bordermatrix{\ & a & b & c & d \cr a & 1 & 3 & 1/3 & 1/2  \cr b & 1/3 & 1 & 1/6 & 2 \cr c & 3 & 6 & 1 &1\cr d & 2 & 1/2 & 1 & 1}$$

\vspace{5mm}

The right Perron vector is: $w=[0.184; 0.152; 0.436; 0.227]$, which leads to the ranking: $c>d >a >b$ while the left Perron vector is $v=[0.248, 0.338, 0.105, 0.259]$, which leads to the different ranking of $c>a>d>b$.

\subsection{Logarithmic least squares method}

The Perron vector is not the only continuous vector-valued function of positive reciprocal matrices that yields the correct scale when the matrix is consistent. Another popular method that also meets these criteria, introduced in \cite{crawford}, is the geometric mean method, or more commonly known as logarithmic least squares method (LLSM). This approach minimizes the multiplicative error; that is, if $p$ denotes the LLSM vector, then this method aims to minimize $e_{ij}$ below:

$$a_{ij}=\frac{p_i}{p_j}e_{ij}$$

That is, LLSM assumes the multiplicative error and aims to minimize the sums of these errors:

$$\displaystyle \sum_{i,j=1}^n \big(\ln{a_{ij}}-\ln{\frac{p_i}{p_j}}\big)^2$$


The normalized LLSM vector is given by:

$$p_i= \frac{\displaystyle\Big(\prod_{j=1}^n a_{ij}\Big)^{1/n}}{\displaystyle \sum_{i=1}^{n}\Big(\prod_{j=1}^n a_{ij}\Big)^{1/n}}$$

The list of relative pros and cons of Perron eigenvalue method versus the LLSM remains long and is a topic of debate. 

\chapter{Ranking with Offense and Defense}

\section{Introduction}
Consider a network of players in a series of one-on-one competitions.  Suppose
the resources for these competitions are limited so that we do not expect
round-robin style matches.  For such a series of unevenly matched competitions
we are interested in an overall ranking of the players.  In particular, we want
to avoid giving preference to an undefeated player if they played fewer games,
or if they played only lower ranked players.  We are interested in using a
``pairwise dominance'' relationship between each pair of players to compute a
total ranking of the players.

The procedure for organizing the outcome data into a collection of pairwise
dominance relationships results in a set of pairwise scores $s_{uv}$ describing
player $u's$ relative strength over player $v$.  These scores can be organized
in a matrix $(S)_{uv} = s_{uv}$.

Many algorithms have been proposed for summarizing the pairwise dominance scores
into a single definitive ranking of all players.  Ranking by the Perron vector
was popularized by Saaty and is based on the following definition of ranking.
Let $r = \{r_i\}_{i=1}^n$ be an absolute score assignment over the $n$ players
in the network.  Then $r$ is a good ranking if $r(u)/r(v)$ is close the pairwise
dominance of player $u$ over player $v$, $s_{uv}$.  Other single dimensional
rankings have been proposed (LLSM, etc).

Given the natural asymmetry of the problem, however, we propose a new ranking
which takes into account two ``scores.'' We will make these notions more precise
in the next section.

\subsection{Youla Decomposition for Skew Symmetric Matrices}

\begin{thm} Let $A$ be a real, skew-symmetric matrix. Then there exists a real, orthogonal matrix $V$ such that 
$$V^TAV=T$$

where $T=T_1 \oplus \dots \oplus T_k \oplus 0\oplus \dots \oplus 0$  where rank of $A$ is $2k$ and the number of zeros are $n-2k$ and 

$$T_j=\begin{pmatrix} 0 & \lambda_j \\ -\lambda_j & 0 \end{pmatrix}$$

for $j=1, \dots, k$. Alternatively written, we have:

$$T=\begin{pmatrix} 0 & \lambda_1\\ -\lambda_1 & 0 \\ &  & 0 & \lambda_2 \\ & & -\lambda_2 & 0 \\ & & & & \ddots \\ & & & & & 0 & \lambda_k \\ & & &  & & -\lambda_k & 0 \\ & & & & & & & 0 \\ & & & & & & & & \ddots \\ & & &  & & & &  & & 0  \end{pmatrix}$$ 


 $V$ may be chosen to that $\lambda_j$ are the singular values of $A$ (alternatively stated, $\lambda_j$ is the positive imaginary part of eigenvalue $i\lambda_j$). More generally, depending on the choice of $V$, we have:

$$T_j=\begin{pmatrix} 0 & z_j \\ -z_j & 0 \end{pmatrix}$$

for some real positive $z_j$.

\end{thm}

This decomposition is called the Youla decomposition and was first derived in \cite{youla}. As for how to actually compute $T$, that can be found in Appendix A of \cite{wimmer}. Presumably using methods in this paper, a user on MathStackExchange posted \cite{chip} a MATLAB function which tridiagonalizes a real skew-symmetric matrix (however, note that this particular algorithm does not yield non-zero entries $\lambda_j$ that are the singular values of $A$, rather, it just yields some positive $z_j$).

\subsection{Rewriting the Youla Decomposition}

\begin{thm} Let $A$ be a real, skew-symmetric matrix with Youla decomposition $VAV^T=T$ where $V$ is a real and orthogonal and $T=T_1 \oplus \dots \oplus T_k \oplus 0\oplus \dots \oplus 0$  where rank of $A$ is $2k$ and the number of zeros are $n-2k$ and 

$$T_j=\begin{pmatrix} 0 & \lambda_j \\ -\lambda_j & 0 \end{pmatrix}$$

Then $A$ may be written as:

$$A=\displaystyle \sum_{j=1}^n \lambda_j(v_{2j-1}v_{2j}^T-v_{2j}v_{2j-1}^T)$$

where $v_j$ denotes column $j$ of $V$. Furthermore, we claim that the vector $v_{2j-1} + iv_{2j}$ is an eigenvector of $A$ associated with eigenvalue $i\lambda_j$. 
 
\end{thm}

\begin{proof}

Consider the Youla decomposition $A=VTV^T$, where $v_j$ denotes column $j$ of $V$. 

$$\begin{array}{lcl} A &=& \begin{pmatrix} v_1 & v_2 & \dots & v_n \end{pmatrix} \begin{pmatrix} 0 & \lambda_1\\ -\lambda_1 & 0 \\ &  & 0 & \lambda_2 \\ & & -\lambda_2 & 0 \\ & & & & \ddots \\ & & & & & 0 & \lambda_k \\ & & &  & & -\lambda_k & 0 \\ & & & & & & & 0 \\ & & & & & & & & \ddots \\ & & &  & & & &  & & 0  \end{pmatrix} \begin{pmatrix} v_1^T \\ v_2^T \\ \vdots \\ v_n^T \end{pmatrix} \\ &=& \begin{pmatrix} -\lambda_1v_2 & \lambda_1v_1 & -\lambda_2v_4 & -\lambda_2v_3 &\dots & -\lambda_kv_{2k} & \lambda_kv_{2k-1} & 0 & \dots & 0  \end{pmatrix} \begin{pmatrix} v_1^T \\ v_2^T \\ \vdots \\ v_n^T \end{pmatrix} \\ \\ &=& -\lambda_1v_2v_1^T+\lambda_1v_1v_2^T-\lambda_2v_4v_3^T+\lambda_2v_3v_4^T -\dots-\lambda_kv_{2k}v_{2k-1}^T+\lambda_kv_{2k-1}v_{2k}^T \\ \\ &=& \lambda_1(v_1v_2^T-v_2v_1^T)+\lambda_2(v_3v_4^T-v_4v_3^T)+\dots+\lambda_k(v_{2k-1}v_{2k}^T-v_{2k}v_{2k-1}^T) \\ \\ &=& \displaystyle \sum_{j=1}^n \lambda_j(v_{2j-1}v_{2j}^T-v_{2j}v_{2j-1}^T) \end{array}$$

Next, to show $v_{2j-1}+iv_{2j}$ is indeed an eigenvector of $A$ associated with $i\lambda_j$, again consider the Youla decomposition with $AV=VT$:

$$\begin{array}{lcl} A \begin{pmatrix} v_1 & v_2 & \dots & v_n \end{pmatrix} &=& \begin{pmatrix} v_1 & v_2 & \dots & v_n \end{pmatrix}\begin{pmatrix} 0 & \lambda_1\\ -\lambda_1 & 0 \\ &  & 0 & \lambda_2 \\ & & -\lambda_2 & 0 \\ & & & & \ddots \\ & & & & & 0 & \lambda_k \\ & & &  & & -\lambda_k & 0 \\ & & & & & & & 0 \\ & & & & & & & & \ddots \\ & & &  & & & &  & & 0  \end{pmatrix} \\ \\ \begin{pmatrix} Av_1 & Av_2 & \dots & Av_n \end{pmatrix} &=&  \begin{pmatrix} -\lambda_1v_2 & \lambda_1v_1 & -\lambda_2v_4 & -\lambda_2v_3 &\dots & -\lambda_kv_{2k} & \lambda_kv_{2k-1} & 0 & \dots & 0  \end{pmatrix} \end{array}$$

Equating components, we get:

$$\begin{array}{lcl} Av_{2j-1} &=& -\lambda_j v_{2j} \\ Av_{2j}&=&\lambda_j v_{2j-1} \end{array}$$

Thus:

$$\begin{array}{lcl} Av_{2j-1}+iAv_{2j} &=& -\lambda_j v_{2j}+i \lambda_j v_{2j-1} \\ A(v_{2j-1}+iv_{2j}) &=& i\lambda_j(v_{2j-1}+iv_{2j}) \end{array}$$

\end{proof}

\subsection{Computational examples using the Youla decomposition for ranking}

We saw in the previous section that a skew symmetric matrix $A$ has a special spectral decomposition:

$$A=\displaystyle \sum_{j=1}^n \lambda_j(v_{2j-1}v_{2j}^T-v_{2j}v_{2j-1}^T)$$

If $\lambda_1$ denotes the spectral radius, we are interested in seeing how well the first term in this sum approximates $A$ in real world data sets, i.e. if $A\approx \lambda_1(v_1v_2^T-v_2v_1^T)$.

We consider two data sets using the NBA regular season for 2013-2014. First, we consider a matrix $M$ based on normalized winning differences between teams:

$$M(i,j)=\frac{\mbox{team i's wins over team j}-\mbox{team j's wins over team i}}{\mbox{total number of team i vs team j games}}$$

Second, we consider a matrix $N$ based on normalized point differences between teams:

$$N(i,j)=\frac{\mbox{team i's points scored against team j}-\mbox{team j's points scored against  team i}}{\mbox{total number of team i vs team j games}}$$

First, we compare the above matrices with $\lambda_1(v_1v_2^T-v_2v_1^T)$ using the induced matrix 2-norm. That is, if $||\cdot||_2$ denotes the vector 2-norm, then:

$$||A||_2=\max_{x\not=0} \frac{||Ax||_2}{||x||_2}$$

Recall the fact that the induced matrix 2-norm gives the largest singular value. Since the singular values of a real skew symmetric matrix $A$ are the (non-negative) imaginary parts of the eigenvalues of $A$, we have

$$||A||_2=\sigma_1=\lambda_1$$

\begin{Claim}

Given orthogonal unit vectors $v_1$ and $v_2$, we have:

$$||\lambda_1(v_1v_2^T-v_2v_1^T)||_2=\lambda_1$$

\end{Claim}

\begin{proof}
We show $||(v_1v_2^T-v_2v_1^T)||_2=1$. The singular values of $v_1v_2^T-v_2v_1^T$ are the square roots of the eigenvalues of $(v_1v_2^T-v_2v_1^T)^T(v_1v_2^T-v_2v_1^T)$. Observe:

$$\begin{array}{lcl}
(v_1v_2^T-v_2v_1^T)^T(v_1v_2^T-v_2v_1^T) &=& (v_2v_1^T-v_1v_2^T)(v_1v_2^T-v_2v_1^T) \\ &=& v_2v_2^T(v_1^Tv_1)+v_1v_1^T(v_2^Tv_2) \\ &=& v_2v_2^T+v_1v_1^T
\end{array}$$

The matrix $v_2v_2^T+v_1v_1^T$ has at most rank 2. There are at most 2 nonzero eigenvalues of this matrix. Since $v_2v_2^T$ and $v_1v_1^T$ are projection matrices:

$$\begin{array}{lcl}
(v_2v_2^T+v_1v_1^T)v_1 &=& v_1 \\ (v_2v_2^T+v_1v_1^T)v_2 &=& v_2
\end{array}$$

Thus, the only nonzero eigenvalue of $v_2v_2^T+v_1v_1^T$ is 1. So, the largest singular value of $v_1v_2^T-v_2v_1^T$ is 1.
\end{proof}

Using MATLAB, we find:

\begin{itemize}

\item $||M||_2=||\lambda_1(v_1v_2^T-v_2v_1^T)||_2=\lambda_1=9.91$
\item $||M-\lambda_1(v_1v_2^T-v_2v_1^T)||_2=4.78$

\end{itemize}

Next, in terms of component wise percent difference, we found that on average, the components of matrix $M$ deviated from those in $\lambda_1(v_1v_2^T-v_2v_1^T)$ in magnitude by 38\%. \\

Next, we compare $N$ and $\lambda_1(v_1v_2^T-v_2v_1^T)$.

\begin{itemize}

\item $||N||_2=||\lambda_1(v_1v_2^T-v_2v_1^T)||_2=\lambda_1=0.73$
\item $||N-\lambda_1(v_1v_2^T-v_2v_1^T)||_2=0.30$

\end{itemize}
 
 Here, on average, the components of matrix $M$ deviated from those in $\lambda_1(v_1v_2^T-v_2v_1^T)$ in magnitude by 129\%.
 
  
\section{Our methods}\label{sec:methods}

\subsection{Pairwise dominance}
The first task is organizing a collection of outcomes into the set of pairwise
dominance scores $s_{uv}$.  Suppose players $u$ and $v$  played $\alpha$ games
against each other and that of these $\alpha$ games, player $u$ won $w_u$ games
and player $v$ won $w_v$ games.  Then we will use the the arithmetic mean of
score differences of~\cite{jiang:statistical:mp10}, and define pairwise
dominance $s_{uv}$ as such:
\begin{equation}\label{eq:suv}
S(u,v) = s_{uv} = \frac{w_u - w_v}{\alpha}.
\end{equation}

This can by modeled by a complete directed graph, where the edge $(u,v)$ is
weighted by the score $s_{uv}$.  Then the matrix $S$ is the adjacency matrix of
the network.  We will refer to $S$ as the \emph{dominance matrix} of the
network.

\subsection{A new ranking algorithm}
Given a dominance matrix $A$ of a network, the goal is to construct a single
dimensional hierarchy of players in the network, called a ranking.  As
mentioned, one possible ranking is a vector $r$ such that $r(u)/r(v)$ is a good
approximation of $S(u,v)$.  This intuitively suggests that, as the $(u,v)^{th}$
entry in $S$ represents the relative strength of member $u$ to member $v$, an
appropriate ranking should respect this relationship.

In this work, we suggest a new notion of ranking.  Namely, we generalize the
idea of dominance and consider facets of strength: offensive strength and
defensive strength.

\begin{defn}
Let $S$ be a dominance matrix of a network.  Then we say that the network has a
\emph{total hierchical ranking} if there exist vectors $\omega, \delta: V
\rightarrow \R$ such that
\begin{equation}\label{eq:offdef}
S(u,v) = c(\omega(u)\delta(v) - \delta(u)\omega(v)) + \epsilon_{uv},
\end{equation}
for some constant $c$ and some allowable error $\epsilon_{uv}$.  We will refer
to $\omega$ and $\delta$ as the \emph{offensive} ranking and $\emph{defensive}$
ranking, respectively.
\end{defn}

\begin{algorithm}[H]
\floatname{algorithm}{Algorithm}
\caption{TotalHierarchicalRanking($S$)}
\label{alg:thr}
input: a dominance matrix $S$\\
output: offensive ranking vector $\omega$, defensive ranking vector $\delta$\\
\begin{algorithmic}
  \State $\lambda=ir \gets$ largest eigenvalue of $S$
  \State $f=a+ib \gets$ right eigenvector of $S$ corresponding to $\lambda$
  \State $\omega \gets a$, $\delta \gets b$\\
  \Return $\omega, \delta$
\end{algorithmic} 
\end{algorithm}

\begin{thm}\label{thm:thr}
Let $G$ be a network with a total hierarchical ranking and let $S$ be the
dominance matrix of $G$.  Then the vectors $\omega, \delta$ returned by
TotalHierarchicalRanking($S$) are offensive and defensive ranking vectors.
\end{thm}

\begin{proof}
We first note that the matrix $S$ is skew-symmetric, i.e., $S = -S^T$, by our
definition of pairwise dominance (\ref{eq:suv}).  Thus, the eigenvalues of $S$
are all of the form $\lambda = \pm ir$ for $r\in\R$, and $f$ must be a complex
vector of the form $f = a + ib$.  Let's consider the spectral decomposition of
$S$,
\begin{equation}\label{eq:specdecomp}
S = \sum_j \lambda_j f_jf_j^*,
\end{equation}
where $f^*$ denotes the conjugate transpose.  Each term of the series is given
by
\begin{align*}
(\lambda f f^*)(u,v) &= ir((a+ib)(u)(a-ib)(v))\\
&= ir(a(u)a(v) + b(u)b(v) - ia(u)b(v) + ib(u)a(v))\\
&= r(a(u)b(v) - b(u)a(v)) + ir(a(u)a(v) + b(v)b(v)).
\end{align*}

With this, (\ref{eq:specdecomp}) can be rewritten:
\begin{equation*}
S(u,v) = \sum_j r_j(a_j(u)b_j(v) - b_j(u)a_j(v)) + ir_j(a_j(u)a_j(v) +
b_j(v)b_j(v)).
\end{equation*}

Since eigenvalues come in conjugate pairs, we can simplify (\ref{eq:specdecomp})
using the above
\begin{align}
S(u,v) &= \sum_{j=1}^{n/2} \lambda_j f_jf_j^* - \lambda_j
\bar{f_j}f_j^T\nonumber\\
&= \sum_{j=1}^{n/2} 2r_j(a_j(u)b_j(v) - b_j(u)a_j(v))\label{imag}\\
&= 2r_1(a_1(u)b_1(v) - b_1(u)a_1(v)) + \sum_{j=2}^{n/2} 2r_j(a_j(u)b_j(v) -
b_j(u)a_j(v))\nonumber\\
\end{align}
where (~\ref{imag}) is a result of the imaginary parts cancelling.  Since we
assume $G$ has a total hierarchical ranking, we know there exist some vectors
$\omega$, $\delta$ such that (\ref{eq:offdef}) holds, so if we show that
$\sum_{j=2}^{n/2} 2r_j(a_j(u)b_j(v) - b_j(u)a_j(v))$ is reasonably small, we may
choose $\omega = a$ and $\delta = b$ to complete the proof.

\end{proof}

\chapter{PageRank and Heat Kernel PageRank}

(This chapter is forthcoming)

\section{PageRank}

\begin{defn}

$$\alpha \displaystyle \sum_{k=0}^{\infty} (1-\alpha)^k \frac{A^ke}{e^TA^ke}$$

\end{defn}

\section{Heat Kernel PageRank}

\begin{defn} 

$$e^t \displaystyle \sum_{k=0}^{\infty} \frac{t^k}{k!} \ \frac{A^ke}{e^TA^ke}$$

\end{defn}

\begin{thebibliography}{1}

  \bibitem{analytic} T. Saaty, {\em The analytic hierarchy process}, McGraw-Hill, New York, 1980. 
  
  \bibitem{perroninsight} T. Saaty, Rank according to Perron: a new insight, {\em Mathematics Magazine}, {\bf 60}(4) (1987) 211-212.

  \bibitem{hornandjohnson} R. Horn and C. Johnson, {\em Matrix Analysis}, Cambridge University Press, Cambridge, UK, 1999.

  \bibitem{johnson} C. Johnson, W. Beine, T. Wang, Right-left asymmetry in an eigenvector ranking procedure, {\em Journal of Mathematical Psychology}, {\bf 19} (1979) 61-64.
  
  \bibitem{crawford} G. Crawford and C. Williams, A note on the analysis of subjective judgement matrices, {\em Journal of Mathematical Psychology} {\bf 29} (1985) 387-405
  
  \bibitem{jiang:statistical:mp10} Xiaoye Jiang and Lek-Heng Lim and Yuan Yao and
Yinyu Ye, Statistical ranking and combinatorial Hodge theory, \emph{Mathematical
Programming}, \textbf{127} 1, (2011) 203--244. 

\bibitem{youla} D.C. Youla, A normal form for a matrix under the unitary congruence group, {\em Canad. J. Math}. {\bf 13} (1960) 694-704.

\bibitem{wimmer} M. Wimmer, Effcient numerical computation of the Pfaffan for dense and banded skew-symmetric matrices, http://arxiv.org/pdf/1102.3440.pdf
\bibitem{chip} http://math.stackexchange.com/questions/167008/block-diagonalizing-an-antisymmetric-matrix
 
  %\bibitem{directedlaplacian} F. Chung, Laplacians and the Cheeger inequality for directed graphs, {\em Annals of Combinatorics}, 9 (2005), no. 1, p. 1-19. 
  
  %\bibitem{tarjan} F. Chung, M. Garey, and R. Tarjan, Strongly connected orientations of mixed multigraphs, {\em Networks} 15 (1985), no. 4, 477-484.
  
  %\bibitem{robbins} H.E. Robbins, A theorem on graphs, with an application to a problem of traffic control, {\em Am. Math. Monthly}, {\bf 46} (1939) 281-283.
  
  %\bibitem{tindell} F. Boesch and R. Rindell, Robbin's theorem for mixed multi graphs, Am. {\em Am. Math Monthly}, {\bf 87}, (1980) 716-719.
   
 % \bibitem{chunglu} F. Chung and L. Lu, {\em Complex Graphs and Networks}, AMS Publications, 2006.
  
 % \bibitem{chunglu2} F. Chung and L. Lu, Connected components in random graphs with given expected degree sequences, {\em Annals of Combinatorics} 6 (2002), no. 2, 125-145.

% \bibitem{radcliffe} F. Chung and M. Radcliffe, On the spectra of general random graphs, {\em Electronic Journal of Combinatorics}, 18(1), (2011).

  %\bibitem{sudakov} M. Krivelevich and B. Sudakov, Pseudo-random graphs, in: {\em More Sets, Graphs, and Numbers}, Bolyai Society Mathematical Studies 15, Springer, (2006), p 199-262.


  \end{thebibliography}


\end{document}
