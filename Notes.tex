\documentclass[a4,11pt,twoside,leqno]{report}
\setlength{\topmargin}{0cm} \setlength{\textheight}{215mm}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}\setlength{\textwidth}{160mm}

\usepackage{amsfonts}
\usepackage{amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage[latin1]{inputenc}
\usepackage[mathscr]{euscript}
\usepackage{tabularx}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem*{ex}{Example}
\numberwithin{equation}{section}
\newtheorem*{Claim}{Claim}

\newcommand{\R}{\mathbb{R}}

\begin{document}

\title{Notes for Research Project on Ranking}
\author{Sinan Aksoy, Fan Chung, Olivia Simpson}


\maketitle

\tableofcontents


\chapter{Background}

Here, we give some background on our problem. We start by listing some of the basic properties of the matrices we will be considering. Then, we briefly consider two popular ranking methods: the Perron eigenvalue method and the logarithmic least squares method. 

\section{Relevant Matrices}
\subsection{Positive matrices}


\begin{thm}[Perron-Frobenius \cite{analytic} \cite{hornandjohnson}] Let $A>0$. Then:
\begin{enumerate}
\item There is a positive (column) vector $w$ such that $Aw=\rho(A)w$. We call $w$ the {\em right Perron vector} of $A$. Similarly, there is a positive (row) vector $v$ such that $vA=\rho(A)v$. We call $v$ the {\em left Perron vector} of $A$. 
\item The right Perron vector $w$ is orthogonal to all other eigenvectors in right eigenspace; similarly, $v$ is orthogonal to all other eigenvectors in left eigenspace. 
\item $\rho(A)$ is an algebraically (and hence geometrically) simple eigenvalue of $A$. 
\item $\rho(A)$ is the unique eigenvalue of maximum modulus. 
\item $\rho(A)$ is bounded above by the maximum row sum of $A$ and from below by the minimum row sum of $A$. 
\end{enumerate}

\end {thm}

As an immediate corollary, we can state limiting behavior of positive matrices A in terms of the right and left Perron vectors:

\begin{cor}[\cite{hornandjohnson}] Let $w$ and $v$ be the right and left Perron vectors of $A>0$. Assume $w$ and $v$ are scaled so that $wv=1$ Then:

$$\lim_{m \rightarrow \infty} \Big(\frac{A}{\rho(A)}\Big)^m=vw$$

\end{cor}

Lastly, we note:

\begin{thm}[\cite{analytic}] Let $A>0$. The largest eigenvalue of $A$, $\rho(A)$, increases as any element $a_{ij}$ increases. 
\end{thm}

\subsection{Positive reciprocal matrices}

\begin{defn} $A$ is a positive reciprocal matrix if $a_{ij}>0$ and $a_{ji}=\frac{1}{a_{ij}}$. 

\end{defn}

\begin{thm}[\cite{analytic}] The eigenvalues of a positive reciprocal matrix satisfy:

$$\displaystyle \sum_{\substack{{j,k}\\j\not=k}} \lambda_j\lambda_k=0$$

\end{thm}


\subsection{Positive consistent (reciprocal) matrices}

\begin{defn} A matrix $A$ is said to be consistent if $a_{ij}a_{jk}=a_{ik}$ for all $i,j,k$. In other words, a positive reciprocal matrix is consistent when pairwise dominance relations are transitive. \\

\noindent {\bf Comment}: Using the above definition, along with the fact that $a_{ji}=1/{a_{ij}}$, the consistency condition becomes $a_{ij}a_{jk}a_{ki}=1$. So, one can also define consistency in a graph theoretic sense: if we think of our matrix $A$ as describing a weighted adjacency matrix, then consistency means that the product of the edge weights in any 3-cycle is 1. \\

\noindent {\bf Comment}: Note that all positive consistent matrices are necessarily reciprocal matrices. Assuming $A$ is positive, the definition of consistency implies $a_{ii}=1$, which, in turn, implies $a_{ij}=a_{ii}/a_{ji}=1/a_{ji}$. Thus, a positive, consistent matrix is necessarily a consistent, positive reciprocal matrix (but the converse is not necessarily true). \\

\noindent {\bf Fact}: If $A$ is a consistent positive reciprocal matrix, then the right and left Perron vectors of $A$, $w$ and $v$, are reciprocals of each other up to a multiplicative factor. That is, there exists $c\in \mathbb{R}$ such that:

$$w_i=c\Big(\frac{1}{v_i}\Big)$$

 Next, we note that consistency can be characterized in terms of the largest eigenvalue of a reciprocal matrix:

\begin{thm}[\cite{analytic}] Let $A$ be a positive reciprocal matrix. Then $A$ is consistent if and only if $\rho(A)=n$.

\end{thm}

Consistent, positive reciprocal matrices also satisfy:

\begin{thm}[\cite{analytic}] Let $A$ be a consistent, positive reciprocal matrix. Then:

$$A^k=n^{k-1}A$$

\end{thm}

A natural question that arises is how to measure consistency in matrices that aren't consistent. Several metrics have been proposed - we mention a popular ''consistency index'' below defined by Saaty \cite{analytic}:

\begin{defn} The Consistency Index (C.I.) of a positive reciprocal matrix $A$ is defined as:

$$\mbox{C.I.}=\frac{\rho(A)-n}{n-1}$$

\end{defn}

\noindent {\bf Open Conjecture}: Let $A\in M_n$ be a positive, reciprocal matrix with $n\geq 4$. The right and left Perron vectors of $A$ are reciprocals of each other (up to a multiplicative factor) if and only if $A$ is consistent.  (Note: for $n=2$, any positive reciprocal matrix is consistent. For $n=3$, it has been proven that regardless of consistency, the right and left eigenvectors are reciprocals of each other. As far as I can tell, the conjecture is still unproven for $n\geq4$.) \\

\noindent {\bf Comment}: While it may not be helpful, we note the above conjecture can be alternatively stated in the language of graph theory: Let $A$ be the weighted adjacency matrix of the complete weighted digraph on $n$ vertices for $n\geq 4$. The products of the edge weights of every 3-cycle is 1 if and only if the right and left Perron vectors of $A$ are reciprocals (up to a multiplicative factor) of each other.

\end{defn}

\section{Two ranking methods}

\subsection{Perron eigenvalue method}

Saaty advocates use of the Perron eigenvalue method in which the ranking is given by the Perron vector of $A$. Saaty makes two key points to justify this method. The first is that, when $A$ is consistent, the Perron vector $w$ satisfies:

$$a_{ij}=\frac{w_i}{w_j}$$

This can be easily proved. Since every elements of $A$ can be determined from the first row of $A$, $A$ has rank one and exactly one nonzero eigenvalue. Using the aforementioned fact that $A^2=nA$, and denoting the columns of $A$ as $[a_1,a_2,\dots,a_n]$ we can see:

$$\begin{array}{lcl} A^2&=&A[a_1,a_2,\dots,a_n] \\ &=&[Aa_1,Aa_2,\dots,Aa_n] \\ &=& [na_1,na_2,\dots,na_n] \\ &=& nA\end{array}$$

which tells us the columns of $A$ are all scalar multiples of the (unique) dominant eigenvector. So, for column $a_k$, the Perron vector $w$ satisfies, for some scalar $c$:

$$\begin{array}{lcl}w_i&=&ca_{ik}\\ w_j&=&ca_{jk} \end{array}$$

which, if we take the ratio and use the consistency definition, yields the desired result: 

$$a_{ij}=\frac{w_i}{w_j}$$

The second key justification Saaty makes in favor of using the Perron eigenvalue method concerns perturbation. Namely, he shows that small changes in the entries of the matrix result in small changes in the Perron vector. While these two arguments seem to be the most compelling, Saaty and others also have other arguments, including empirical arguments, on why the Perron vector should be used in ranking. 

\subsubsection{Asymmetry in rankings derived from right and left Perron vectors}

While Saaty and others have traditionally used the right Perron vector for ranking, there seems to be no justification for why the left eigenvector should not be used in place of the right eigenvector. Larger values in the right Perron vector signify a higher ranking for that item whereas smaller values in the left Perron vector signify a higher ranking for that item. In the case where $A$ is consistent, given the above mentioned reciprocal relationship between the right and left Perron vectors, both vectors will lead to the same ranking. However, if $A$ is inconsistent, then the right and left Perron vectors can lead to different rankings. In a recent survey of the Analytic Hierarchy Process, the author listed this issue as the largest theoretical dispute in the field. We give an example below from \cite{johnson} to that illustrates the rank reversal issue:

$$A=\bordermatrix{\ & a & b & c & d \cr a & 1 & 3 & 1/3 & 1/2  \cr b & 1/3 & 1 & 1/6 & 2 \cr c & 3 & 6 & 1 &1\cr d & 2 & 1/2 & 1 & 1}$$

\vspace{5mm}

The right Perron vector is: $w=[0.184; 0.152; 0.436; 0.227]$, which leads to the ranking: $c>d >a >b$ while the left Perron vector is $v=[0.248, 0.338, 0.105, 0.259]$, which leads to the different ranking of $c>a>d>b$.

\subsection{Logarithmic least squares method}

The Perron vector is not the only continuous vector-valued function of positive reciprocal matrices that yields the correct scale when the matrix is consistent. Another popular method that also meets these criteria, introduced in \cite{crawford}, is the geometric mean method, or more commonly known as logarithmic least squares method (LLSM). This approach minimizes the multiplicative error; that is, if $p$ denotes the LLSM vector, then this method aims to minimize $e_{ij}$ below:

$$a_{ij}=\frac{p_i}{p_j}e_{ij}$$

That is, LLSM assumes the multiplicative error and aims to minimize the sums of these errors:

$$\displaystyle \sum_{i,j=1}^n \big(\ln{a_{ij}}-\ln{\frac{p_i}{p_j}}\big)^2$$


The normalized LLSM vector is given by:

$$p_i= \frac{\displaystyle\Big(\prod_{j=1}^n a_{ij}\Big)^{1/n}}{\displaystyle \sum_{i=1}^{n}\Big(\prod_{j=1}^n a_{ij}\Big)^{1/n}}$$

The list of relative pros and cons of Perron eigenvalue method versus the LLSM remains long and is a topic of debate. 

\chapter{Ranking with Offense and Defense}

\section{Introduction}
Consider a network of players in a series of one-on-one competitions.
The resources for these competitions may be limited, and we do not expect
round-robin style matches.  Then, for a series of unevenly matched competitions,
we are interested in an overall ranking of the players knowing the outcome of
each competition.  In particular, we want to avoid giving preference to an
undefeated player if they played fewer games, or if they played only lower
ranked players.  We are interested in using a ``pairwise dominance''
relationship between each pair of players to compute a total ranking of the
players.

The procedure for organizing the outcome data into a collection of pairwise
dominance relationships results in a set of pairwise scores $s_{uv}$ describing
player $u's$ relative strength over player $v$.  These scores can be organized
in a matrix $(S)_{uv} = s_{uv}$.

Many algorithms have been proposed for summarizing the pairwise dominance scores
into a single definitive ranking of all players.  Ranking by the Perron vector
was popularized by Saaty and is based on the following definition of ranking.
Let $r = \{r_i\}_{i=1}^n$ be an absolute score assignment over the $n$ players
in the network.  Then $r$ is a good ranking if $r(u)/r(v)$ is close the pairwise
dominance of player $u$ over player $v$, $s_{uv}$.  Other single dimensional
rankings have been proposed (LLSM, etc).

Given the natural asymmetry of the propblem, however, we propose a new ranking
which takes into account two ``scores.'' We will make these notions more precise
in the next section.

\section{Our methods}\label{sec:methods}

\subsection{Pairwise dominance}
The first task is organizing a collection of outcomes into the set of pairwise
dominance scores $s_{uv}$.  Suppose players $u$ and $v$ competed each other in
$\alpha$ and suppose that of these $\alpha$ games, player $u$ won $w_u$ games
and player $v$ won $w_v$ games.  Then we will use the the arithmetic mean of
score differences of~\cite{jiang:statistical:mp10}, and define pairwise
dominance $s_{uv}$ as such:
\begin{equation}\label{eq:suv}
S(u,v) = s_{uv} = \frac{w_u - w_v}{\alpha}.
\end{equation}

This can by modeled by a complete directed graph, where the edge $(u,v)$ is
weighted by the score $s_{uv}$.  Then the matrix $S$ is the adjacency matrix of
the network.  We will refer to $S$ as the \emph{dominance matrix} of the
network.

\subsection{A new ranking algorithm}
Given a dominance matrix $A$ of a network, the goal is to construct a single
dimensional hierarchy of players in the network, called a ranking.  As
mentioned, one possible ranking is a vector $r$ such that $r(u)/r(v)$ is a good
approximation of $S(u,v)$.  This intuitively suggests that, as the $(u,v)^{th}$
entry in $S$ represents the relative strength of member $u$ to member $v$, an
appropriate ranking should respect this relationship.

In this work, we suggest a new notion of ranking.  Namely, we generalize the
idea of dominance and consider facets of strength: offensive strength and
defensive strength.

\begin{defn}
Let $S$ be a dominance matrix of a network.  Then we say that the network has a
\emph{total hierchical ranking} if there exist vectors $\omega, \delta: V
\rightarrow \R$ such that
\begin{equation}\label{eq:offdef}
S(u,v) = c(\omega(u)\delta(v) - \delta(u)\omega(v)) + \epsilon_{uv},
\end{equation}
for some constant $c$ and some allowable error $\epsilon_{uv}$.  We will refer
to $\omega$ and $\delta$ as the \emph{offensive} ranking and $\emph{defensive}$
ranking, respectively.
\end{defn}

\begin{algorithm}[H]
\floatname{algorithm}{Algorithm}
\caption{TotalHierarchicalRanking($S$)}
\label{alg:thr}
input: a dominance matrix $S$\\
output: offensive ranking vector $\omega$, defensive ranking vector $\delta$\\
\begin{algorithmic}
  \State $\lambda=ir \gets$ largest eigenvalue of $S$
  \State $f=a+ib \gets$ right eigenvector of $S$ corresponding to $\lambda$
  \State $\omega \gets a$, $\delta \gets b$\\
  \Return $\omega, \delta$
\end{algorithmic} 
\end{algorithm}

\begin{thm}\label{thm:thr}
Let $G$ be a network with a total hierarchical ranking and let $S$ be the
dominance matrix of $G$.  Then the vectors $\omega, \delta$ returned by
TotalHierarchicalRanking($S$) are offensive and defensive ranking vectors.
\end{thm}

\begin{proof}
We first note that the matrix $S$ is skew-symmetric, i.e., $S = -S^T$, by our
definition of pairwise dominance (\ref{eq:suv}).  Thus, the eigenvalues of $S$
are all of the form $\lambda = \pm ir$ for $r\in\R$, and $f$ must be a complex
vector of the form $f = a + ib$.  Let's consider the spectral decomposition of
$S$,
\begin{equation}\label{eq:specdecomp}
S = \sum_j \lambda_j f_jf_j^*,
\end{equation}
where $f^*$ denotes the conjugate transpose.  Each term of the series is given
by
\begin{align*}
(\lambda f f^*)(u,v) &= ir((a+ib)(u)(a-ib)(v))\\
&= ir(a(u)a(v) + b(u)b(v) - ia(u)b(v) + ib(u)a(v))\\
&= r(a(u)b(v) - b(u)a(v)) + ir(a(u)a(v) + b(v)b(v)).
\end{align*}

With this, (\ref{eq:specdecomp}) can be rewritten:
\begin{equation*}
S(u,v) = \sum_j r_j(a_j(u)b_j(v) - b_j(u)a_j(v)) + ir_j(a_j(u)a_j(v) +
b_j(v)b_j(v)).
\end{equation*}
By our assumption, there exist vectors $\omega, \delta$ such that
(\ref{eq:offdef}) hold.  So we take the real part of the dominant eigenpair.


\end{proof}

\chapter{PageRank and Heat Kernel PageRank}

(This chapter is forthcoming)

\section{PageRank}

\begin{defn}

$$\alpha \displaystyle \sum_{k=0}^{\infty} (1-\alpha)^k \frac{A^ke}{e^TA^ke}$$

\end{defn}

\section{Heat Kernel PageRank}

\begin{defn} 

$$e^t \displaystyle \sum_{k=0}^{\infty} \frac{t^k}{k!} \ \frac{A^ke}{e^TA^ke}$$

\end{defn}

\begin{thebibliography}{1}

  \bibitem{analytic} T. Saaty, {\em The analytic hierarchy process}, McGraw-Hill, New York, 1980. 
  
  \bibitem{perroninsight} T. Saaty, Rank according to Perron: a new insight, {\em Mathematics Magazine}, {\bf 60}(4) (1987) 211-212.

  \bibitem{hornandjohnson} R. Horn and C. Johnson, {\em Matrix Analysis}, Cambridge University Press, Cambridge, UK, 1999.

  \bibitem{johnson} C. Johnson, W. Beine, T. Wang, Right-left asymmetry in an eigenvector ranking procedure, {\em Journal of Mathematical Psychology}, {\bf 19} (1979) 61-64.
  
  \bibitem{crawford} G. Crawford and C. Williams, A note on the analysis of subjective judgement matrices, {\em Journal of Mathematical Psychology} {\bf 29} (1985) 387-405
  
  \bibitem{jiang:statistical:mp10} Xiaoye Jiang and Lek-Heng Lim and Yuan Yao and
Yinyu Ye, Statistical ranking and combinatorial Hodge theory, \emph{Mathematical
Programming}, \textbf{127} 1, (2011) 203--244. 
  
  %\bibitem{directedlaplacian} F. Chung, Laplacians and the Cheeger inequality for directed graphs, {\em Annals of Combinatorics}, 9 (2005), no. 1, p. 1-19. 
  
  %\bibitem{tarjan} F. Chung, M. Garey, and R. Tarjan, Strongly connected orientations of mixed multigraphs, {\em Networks} 15 (1985), no. 4, 477-484.
  
  %\bibitem{robbins} H.E. Robbins, A theorem on graphs, with an application to a problem of traffic control, {\em Am. Math. Monthly}, {\bf 46} (1939) 281-283.
  
  %\bibitem{tindell} F. Boesch and R. Rindell, Robbin's theorem for mixed multi graphs, Am. {\em Am. Math Monthly}, {\bf 87}, (1980) 716-719.
   
 % \bibitem{chunglu} F. Chung and L. Lu, {\em Complex Graphs and Networks}, AMS Publications, 2006.
  
 % \bibitem{chunglu2} F. Chung and L. Lu, Connected components in random graphs with given expected degree sequences, {\em Annals of Combinatorics} 6 (2002), no. 2, 125-145.

% \bibitem{radcliffe} F. Chung and M. Radcliffe, On the spectra of general random graphs, {\em Electronic Journal of Combinatorics}, 18(1), (2011).

  %\bibitem{sudakov} M. Krivelevich and B. Sudakov, Pseudo-random graphs, in: {\em More Sets, Graphs, and Numbers}, Bolyai Society Mathematical Studies 15, Springer, (2006), p 199-262.


  \end{thebibliography}


\end{document}
